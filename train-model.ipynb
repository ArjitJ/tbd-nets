{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to train a Transparency by Design network (TbD-net) on the CLEVR dataset. Training for CoGenT can be done in much the same manner, simply changing the training and validation loaders and updating the evaluation code to evaluate over Condition A and Condition B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from utils.clevr import load_vocab, ClevrDataLoaderNumpy, ClevrDataLoaderH5\n",
    "from tbd.module_net import TbDNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run on CUDA if we have CUDA support so that we can train our model in a reasonable amount of time. We'll define the device accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create our model.\n",
    "\n",
    "We first load the `vocab` file, which holds a list of all the modules we're going to need. We then pass the vocab into our `TbDNet`, which creates an appropriate neural module for each operation in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab(Path('data/vocab.json'))\n",
    "\n",
    "# to train on 14x14 feature maps, pass feature_dim=(1024, 14, 14) to TbDNet\n",
    "tbd_net = TbDNet(vocab, feature_dim=(1024, 14, 14)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we need to load our data.\n",
    "\n",
    "The `use_numpy_format` flag indicates whether we're loading data using npy files or HDF5 files.\n",
    "\n",
    "We create the appropriate `DataLoader` objects depending on the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features from  data/training/train_features.h5\n",
      "Reading features from  data/validation/val_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tbd-nets/utils/clevr.py:352: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  with h5py.File(question_h5_path) as question_h5:\n"
     ]
    }
   ],
   "source": [
    "use_numpy_format = False\n",
    "\n",
    "if use_numpy_format:\n",
    "    train_loader_kwargs = {\n",
    "        'question_np': Path('data/training/train_questions.npy'),\n",
    "        'image_idx_np': Path('data/training/train_image_idxs.npy'),\n",
    "        'program_np': Path('data/training/train_programs.npy'),\n",
    "        'feature_np': Path('data/train_features_hres.npy'),\n",
    "        'answer_np': Path('data/training/train_answers.npy'),\n",
    "        'batch_size': 128,\n",
    "        'num_workers': 2,\n",
    "        'shuffle': True\n",
    "    }\n",
    "\n",
    "    val_loader_kwargs = {\n",
    "        'question_np': Path('data/validation/val_questions.npy'),\n",
    "        'image_idx_np': Path('data/validation/val_image_idxs.npy'),\n",
    "        'program_np': Path('data/validation/val_programs.npy'),\n",
    "        'feature_np': Path('clevr-iep/data/val_features_hres.npy'),\n",
    "        'answer_np': Path('data/validation/val_answers.npy'),\n",
    "        'batch_size': 128,\n",
    "        'num_workers': 2,\n",
    "        'shuffle': False\n",
    "    }\n",
    "    \n",
    "    train_loader = ClevrDataLoaderNumpy(**train_loader_kwargs)\n",
    "    val_loader = ClevrDataLoaderNumpy(**val_loader_kwargs)\n",
    "else:\n",
    "    train_loader_kwargs = {\n",
    "        'question_h5': Path('data/training/train_questions.h5'),\n",
    "        'feature_h5': Path('data/training/train_features.h5'),\n",
    "        'batch_size': 128,\n",
    "        'num_workers': 2,\n",
    "        'shuffle': True\n",
    "    }\n",
    "    val_loader_kwargs = {\n",
    "        'question_h5': Path('data/validation/val_questions.h5'),\n",
    "        'feature_h5': Path('data/validation/val_features.h5'),\n",
    "        'batch_size': 128,\n",
    "        'num_workers': 2,\n",
    "        'shuffle': False\n",
    "    }\n",
    "    \n",
    "    train_loader = ClevrDataLoaderH5(**train_loader_kwargs)\n",
    "    val_loader = ClevrDataLoaderH5(**val_loader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer format we prefer is different from that of Justin Johson *et al.*\n",
    "\n",
    "Their answers are sorted according to ASCII value, while we group by answer type and sort these alphabetically. Note that we have fewer answers than them, because we do not list the special tokens `<NULL>`, `<START>`, `<END>`, and `<UNK>` as answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ans(answers):\n",
    "    ''' Map the answers from the format Justin Johnson et al. use to our own format. '''\n",
    "    ans_tensor = torch.LongTensor(answers.size())\n",
    "    ans_tensor[answers == 15] = 0  # blue\n",
    "    ans_tensor[answers == 16] = 1  # brown\n",
    "    ans_tensor[answers == 18] = 2  # cyan\n",
    "    ans_tensor[answers == 20] = 3  # gray\n",
    "    ans_tensor[answers == 21] = 4  # green\n",
    "    ans_tensor[answers == 25] = 5  # purple\n",
    "    ans_tensor[answers == 26] = 6  # red\n",
    "    ans_tensor[answers == 30] = 7  # yellow\n",
    "    ans_tensor[answers == 17] = 8  # cube\n",
    "    ans_tensor[answers == 19] = 9  # cylinder\n",
    "    ans_tensor[answers == 29] = 10 # sphere\n",
    "    ans_tensor[answers == 22] = 11 # large\n",
    "    ans_tensor[answers == 28] = 12 # small\n",
    "    ans_tensor[answers == 23] = 13 # metal\n",
    "    ans_tensor[answers == 27] = 14 # rubber\n",
    "    ans_tensor[answers == 24] = 15 # no\n",
    "    ans_tensor[answers == 31] = 16 # yes\n",
    "    ans_tensor[answers == 4] = 17  # 0\n",
    "    ans_tensor[answers == 5] = 18  # 1\n",
    "    ans_tensor[answers == 6] = 19  # 10 <- originally sorted by ASCII value, not numerical\n",
    "    ans_tensor[answers == 7] = 20  # 2\n",
    "    ans_tensor[answers == 8] = 21  # 3\n",
    "    ans_tensor[answers == 9] = 22  # 4\n",
    "    ans_tensor[answers == 10] = 23 # 5\n",
    "    ans_tensor[answers == 11] = 24 # 6\n",
    "    ans_tensor[answers == 12] = 25 # 7\n",
    "    ans_tensor[answers == 13] = 26 # 8\n",
    "    ans_tensor[answers == 14] = 27 # 9\n",
    "    return ans_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each epoch, we'll save our model's state, the epoch, and the training state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, filename):\n",
    "    ''' Save the training state. '''\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': tbd_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader):\n",
    "    '''\n",
    "    Convenience function for checking the accuracy of the model \n",
    "    over all the data in a given `DataLoader`\n",
    "    '''\n",
    "    torch.set_grad_enabled(False)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for batch in loader:\n",
    "        _, _, feats, answers, programs = batch\n",
    "        feats = feats.to(device)\n",
    "        programs = programs.to(device)\n",
    "\n",
    "        outs = tbd_net(feats, programs)\n",
    "        _, preds = outs.max(1)\n",
    "        mapped_ans = map_ans(answers)\n",
    "        num_correct += (preds.to('cpu') == mapped_ans).sum()\n",
    "        num_samples += preds.size(0)\n",
    "\n",
    "    return (num_correct, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_acc(f, string, correct, total):\n",
    "    ''' Convenience function to write the accuracy to a file '''\n",
    "    percent = correct / total * 100.0 if total != 0 else 100\n",
    "    f.write(string.format(correct, total, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the optimizer and loss function we will use to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(tbd_net.parameters(), 1e-04)\n",
    "xent_loss = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch():\n",
    "    ''' Convenience function to train the model for one epoch.\n",
    "    Note that the reported training accuracy is not a true measure of accuracy after\n",
    "    training for a full epoch, as it is computed during training. However, it does give a\n",
    "    decent measure of progress.\n",
    "    '''\n",
    "    torch.set_grad_enabled(True)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for batch in train_loader:\n",
    "        t = time.time()\n",
    "        _, _, feats, answers, programs = batch\n",
    "        feats = feats.to(device)\n",
    "        programs = programs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outs = tbd_net(feats, programs)\n",
    "        _, preds = outs.max(1)\n",
    "        mapped_ans = map_ans(answers)\n",
    "        num_correct += (preds.to('cpu') == mapped_ans).sum()\n",
    "        num_samples += preds.size(0)\n",
    "        loss = xent_loss(outs, mapped_ans.to(device))\n",
    "    \n",
    "        # The following line applies an L1 penalty to the intermediate attention masks.\n",
    "        # This gives a signal to the model to minimize its attention outputs, which we \n",
    "        # find (nearly) removes spurious activations in background regions. Our thought\n",
    "        # process is that background regions normally get no signal to push them to zero\n",
    "        # because the model is able to effectively ignore them, so a somewhat noisy\n",
    "        # attention output is adequate. By applying this penalty, we incentivize minimizing\n",
    "        # the spurious activations, leading to better-looking outputs. For a comparison, see\n",
    "        # the visualize-outputs.ipynb notebook. If you wish to replicate the *original*\n",
    "        # results, comment the following line or change the multiplier to zero. In all \n",
    "        # other cases, we recommend keeping this factor and tuning it to your use-case if\n",
    "        # this default is not adequate.\n",
    "        loss += tbd_net.attention_sum * 2.5e-07\n",
    "        \n",
    "        loss_file.write('Loss: {}\\n'.format(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(time.time() -t)\n",
    "\n",
    "    val_correct, val_samples = check_accuracy(val_loader)\n",
    "    \n",
    "    write_acc(acc_file, 'Train Accuracy: {} / {} ({:.2f}%)\\n', num_correct, num_samples)\n",
    "    write_acc(acc_file, 'Val Accuracy: {} / {} ({:.2f}%)\\n', val_correct, val_samples)\n",
    "    acc_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a loss log and an accuracy log for writing, then train for 30 epochs, saving our model after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.356304883956909\n",
      "4.098412036895752\n",
      "4.052089214324951\n",
      "3.956308603286743\n",
      "3.9451968669891357\n",
      "3.7481513023376465\n",
      "3.800499439239502\n",
      "3.9841885566711426\n",
      "4.166390657424927\n",
      "3.968914031982422\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6d744c30dbab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bce36ca6f187>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_file = open(Path('results/example-loss.txt'), 'a')\n",
    "acc_file = open(Path('results/example-acc.txt'), 'a')\n",
    "epoch = 0\n",
    "import time \n",
    "while epoch < 30:\n",
    "    save_checkpoint(epoch, 'example-{:02d}.pt'.format(epoch))\n",
    "    epoch += 1\n",
    "    print('starting epoch', epoch)\n",
    "    t = time.time()\n",
    "    train_epoch()\n",
    "    print(time.time()-t)\n",
    "\n",
    "save_checkpoint(epoch, 'example-{:02d}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's see how our model did!\n",
    "\n",
    "You can certainly use the following in a separate notebook to monitor progress during training, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, title):\n",
    "    ''' Scatterplot with a smoothed average line '''\n",
    "    fig, ax = plt.subplots()\n",
    "    x = range(len(data))\n",
    "    ax.scatter(x, data, s=5)\n",
    "    c = np.cumsum(np.insert(data, 0, 0))\n",
    "    smoothed = (c[50:] - c[:-50]) / 50\n",
    "    ax.plot(smoothed, color='r')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in our batch losses and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "with open(Path('results/example-loss.txt')) as f:\n",
    "    for line in f:\n",
    "        loss.append(float(line[6:]))\n",
    "loss = np.array(loss)\n",
    "\n",
    "plot(loss, 'Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loss curve looks pretty typical for our model.\n",
    "\n",
    "We get a very quick drop in the loss as our model learns some answers are ridiculous. For example, the `query_color` module should never output an answer that isn't a color and `count` should never output an answer that's not a number.\n",
    "\n",
    "We then begin learning useful attentions, which lead to a rapid decrease in the loss, following which we see a smooth decrease toward zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in our accuracy next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "with open(Path('results/example-acc.txt')) as f:\n",
    "    for line in f:\n",
    "        train_acc.append(float(line[-8:-3]))\n",
    "        val_acc.append(float(f.readline()[-8:-3]))\n",
    "    \n",
    "train_acc = np.array(train_acc)\n",
    "val_acc = np.array(val_acc)\n",
    "\n",
    "x = range(len(train_acc))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, train_acc, label='Train', c='g')\n",
    "ax.plot(x, val_acc, label='Val', c='g', ls=':')\n",
    "ax.legend()\n",
    "\n",
    "print(train_acc)\n",
    "print(val_acc)\n",
    "print('Highest validation accuracy: {}'.format(val_acc.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see our model learns remarkably quickly. After the first couple epochs, the model is refining attentions, having learned generally what they should be doing. Again, note that the training accuracy is not quite a correct measure of training performance since it is computed during training. This is why training accuracy is slightly below validation accuracy for a while."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
